{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HEpNLQ2CLbw",
        "outputId": "076105c2-89f5-4d52-d830-35a6a5ea5773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/e7/59fcd3501f47b7a661a69e15ef417463fbc88dd334d9af2d9d6685710038/nilearn-0.7.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 9.2MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/site-packages (from nilearn) (1.3.3)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.6/site-packages (from nilearn) (2.22.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/site-packages (from nilearn) (0.25.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/site-packages (from nilearn) (0.22)\n",
            "Collecting nibabel>=2.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/bf/ba089fec67237f6439c345b8977ca6dde67402ada6592bf84c2c78d557ff/nibabel-3.2.1-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 40.3MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/site-packages (from nilearn) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/site-packages (from nilearn) (1.17.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests>=2->nilearn) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests>=2->nilearn) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests>=2->nilearn) (2.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/site-packages (from pandas>=0.18.0->nilearn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.18.0->nilearn) (2019.3)\n",
            "Collecting packaging>=14.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/89/7ea760b4daa42653ece2380531c90f64788d979110a2ab51049d92f408af/packaging-20.9-py2.py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 7.9MB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.18.0->nilearn) (1.13.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging>=14.3->nibabel>=2.0.2->nilearn) (2.4.5)\n",
            "Installing collected packages: packaging, nibabel, nilearn\n",
            "Successfully installed nibabel-3.2.1 nilearn-0.7.0 packaging-20.9\n",
            "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting dipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ed/b330f0b0bc7c03c8ecba77efb366a9eb5950a3b4ed6dfb1c8a4d4c106b6f/dipy-1.3.0-cp36-cp36m-manylinux2010_x86_64.whl (7.7MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7MB 6.0MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.6/site-packages (from dipy) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.6/site-packages (from dipy) (4.40.0)\n",
            "Requirement already satisfied: h5py>=2.5.0 in /usr/local/lib/python3.6/site-packages (from dipy) (2.10.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/site-packages (from dipy) (1.3.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.6/site-packages (from nibabel>=3.0.0->dipy) (20.9)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/site-packages (from nibabel>=3.0.0->dipy) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from h5py>=2.5.0->dipy) (1.13.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging>=14.3->nibabel>=3.0.0->dipy) (2.4.5)\n",
            "Installing collected packages: dipy\n",
            "Successfully installed dipy-1.3.0\n",
            "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/site-packages (4.3.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/site-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from plotly) (1.13.0)\n",
            "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/site-packages (from sklearn) (0.22)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.3.3)\n",
            "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting git+https://github.com/qubvel/segmentation_models\n",
            "  Cloning https://github.com/qubvel/segmentation_models to /tmp/pip-req-build-zkxyal60\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models /tmp/pip-req-build-zkxyal60\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: keras_applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/site-packages (from segmentation-models==1.0.1) (1.0.8)\n",
            "Collecting image-classifiers==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/81/98/6f84720e299a4942ab80df5f76ab97b7828b24d1de5e9b2cbbe6073228b7/image_classifiers-1.0.0-py3-none-any.whl\n",
            "Collecting efficientnet==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/site-packages (from keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (1.17.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/site-packages (from efficientnet==1.0.0->segmentation-models==1.0.1) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (1.13.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (6.2.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.4)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.1.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.3.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.6.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (4.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (42.0.2)\n",
            "Building wheels for collected packages: segmentation-models\n",
            "  Building wheel for segmentation-models (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for segmentation-models: filename=segmentation_models-1.0.1-cp36-none-any.whl size=33793 sha256=075ba601b15c61831b23b5d07003261a1df84679f533466b32e87e992f6b6eb6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dfbl4snp/wheels/49/cf/46/cbb4bb64518c402aea99df9d466f1081450597e653256bbcf4\n",
            "Successfully built segmentation-models\n",
            "Installing collected packages: image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 segmentation-models-1.0.1\n",
            "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn;\n",
        "!pip install dipy;\n",
        "!pip install plotly;\n",
        "!pip install sklearn;\n",
        "!pip install git+https://github.com/qubvel/segmentation_models;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz2IX2GkCLbz",
        "outputId": "f1609e34-8485-484c-df18-2db2447a0f43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from skimage.exposure import rescale_intensity\n",
        "from keras.callbacks import History\n",
        "from skimage import io\n",
        "import os\n",
        "import numpy as np\n",
        "import nibabel\n",
        "import cv2\n",
        "from heapq import nlargest\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL9PXtfRCLb3"
      },
      "outputs": [],
      "source": [
        "data_path = \"/floyd/home/\"\n",
        "image_rows = 256\n",
        "image_cols = 256\n",
        "\n",
        "# train / validation\n",
        "def create_data(train):  \n",
        "    train_data_path = os.path.join(data_path, f'{train}')\n",
        "    print(train_data_path)\n",
        "    images = os.listdir(train_data_path)\n",
        "\n",
        "    training_masks = []\n",
        "    training_images = []\n",
        "    for i in images:\n",
        "        if i.endswith('.csv'):\n",
        "            pass\n",
        "        \n",
        "        else:\n",
        "            p = os.path.join(train_data_path, i)\n",
        "            im_list = os.listdir(p)\n",
        "            \n",
        "            for j in im_list:\n",
        "                if j.endswith('seg.nii.gz'):\n",
        "                    training_masks.append(j)\n",
        "                    \n",
        "                else:\n",
        "                    training_images.append(j)\n",
        "\n",
        "    N = 4\n",
        "    training_images = [sorted(training_images[n:n+N]) for n in range(0, len(training_images), N)]\n",
        "    \n",
        "    # takes only the 1st element\n",
        "    training_images = [item[0] for item in training_images]\n",
        "    print(\"No of training_images: \", len(training_images))\n",
        "    #training images\n",
        "    imgs_train = [] \n",
        "    #training masks\n",
        "    masks_train = [] \n",
        "    \n",
        "    for tumor, orig in zip(training_masks, training_images):\n",
        "        tumor_folder = '_'.join(tumor.split(\"_\")[0:3])\n",
        "\n",
        "        #load 3D training mask\n",
        "        training_mask = nibabel.load(train_data_path+'/'+tumor_folder +'/'+ tumor)\n",
        "        #load 3D training image\n",
        "        training_image = nibabel.load(train_data_path+'/'+tumor_folder +'/'+ orig) \n",
        "        \n",
        "        for k in range(training_mask.shape[2]):\n",
        "        #axial cuts are made along the z axis with undersampling\n",
        "            mask_2d = np.array(training_mask.get_data()[::2, ::2, k])\n",
        "            image_2d = np.array(training_image.get_data()[::2, ::2, k])\n",
        "            #print(image_2d.shape)\n",
        "        #we recover the 2D sections containing the tumor\n",
        "        #if mask_2d contains only 0, it means that there is no tumor\n",
        "            if len(np.unique(mask_2d)) != 1:\n",
        "                masks_train.append(mask_2d)\n",
        "                imgs_train.append(image_2d)\n",
        "\n",
        "    print(\"imgs_train len: \", len(imgs_train))\n",
        "    print(\"masks_train len: \", len(masks_train))\n",
        "    imgs = np.ndarray((len(imgs_train), image_rows, image_cols), dtype=np.uint8)\n",
        "    imgs_mask = np.ndarray((len(masks_train), image_rows, image_cols), dtype=np.uint8)\n",
        "\n",
        "\n",
        "    for index, img in enumerate(imgs_train):\n",
        "    #print(img.shape)\n",
        "        img = cv2.resize(img, (image_rows, image_rows)) \n",
        "        imgs[index, :, :] = img\n",
        "\n",
        "    for index, img in enumerate(masks_train):\n",
        "        img = cv2.resize(img, (image_rows, image_rows)) \n",
        "        imgs_mask[index, :, :] = img\n",
        "\n",
        "    np.save(f'/floyd/home/imgs_{train}.npy', imgs)\n",
        "    np.save(f'/floyd/home/masks_{train}.npy', imgs_mask)\n",
        "    print('Saving to .npy files done.') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8LpDpaBCLb6"
      },
      "outputs": [],
      "source": [
        "create_data('train')\n",
        "#create_data('validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIrdb9APCLb7",
        "outputId": "816d7679-a235-452b-94bb-a1d6eb74ff02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(24229, 256, 256)\n",
            "(24229, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "# load the data\n",
        "imgs_train_path = '/floyd/home/imgs_train.npy'\n",
        "masks_train_path = '/floyd/home/masks_train.npy'\n",
        "\n",
        "# imgs_val_path = '/floyd/home/imgs_validation.npy'\n",
        "# masks_val_path = '/floyd/home/masks_validation.npy'\n",
        "\n",
        "def load_data(imgs_train_path, masks_train_path):\n",
        "    imgs_train = np.load(imgs_train_path)\n",
        "    masks_train = np.load(masks_train_path)\n",
        "    return imgs_train, masks_train\n",
        "\n",
        "imgs_train, imgs_mask_train = load_data(imgs_train_path, masks_train_path)\n",
        "# check the shape\n",
        "print(imgs_train.shape)\n",
        "print(imgs_mask_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDhhsvOuCLb8"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline \n",
        "testImage = np.array(imgs_train)[100]\n",
        "testImage = cv2.resize(testImage, (256, 256)) \n",
        "print(testImage.shape)\n",
        "print(type(testImage))\n",
        "plt.imshow(testImage, cmap = plt.cm.gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOQZIBkbCLb9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline \n",
        "testImage = np.array(imgs_mask_train)[100]\n",
        "testImage = cv2.resize(testImage, (256, 256)) \n",
        "print(testImage.shape)\n",
        "print(type(testImage))\n",
        "plt.imshow(testImage, cmap = plt.cm.gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJC3Z4ooCLb9",
        "outputId": "4d1248fc-6f9d-4fd7-d5e7-f4916c5852f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19383, 256, 256)\n",
            "(19383, 256, 256)\n",
            "(4846, 256, 256)\n",
            "(4846, 256, 256)\n",
            "-----------------\n",
            "(19383, 256, 256, 1)\n",
            "(19383, 256, 256, 1)\n",
            "(4846, 256, 256, 1)\n",
            "(4846, 256, 256, 1)\n",
            "-----------------\n",
            "(2500, 256, 256, 1)\n",
            "(2500, 256, 256, 1)\n",
            "(2500, 256, 256, 1)\n",
            "(2500, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(imgs_train, imgs_mask_train, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "print(\"-----------------\")\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "y_train = np.expand_dims(y_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "y_test = np.expand_dims(y_test, axis=-1)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "X_train = X_train[0:2500]\n",
        "X_test = X_test[0:2500]\n",
        "y_train = y_train[0:2500]\n",
        "y_test = y_test[0:2500]\n",
        "print(\"-----------------\")\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FY9lcA1CLb-",
        "outputId": "7dd99b71-bc9f-426d-930c-d5ccd1765184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segmentation Models: using `keras` framework.\n"
          ]
        }
      ],
      "source": [
        "import segmentation_models as sm\n",
        "BACKBONE = 'resnet34'\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "X_train = preprocess_input(X_train)\n",
        "X_test = preprocess_input(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYbo_n8kCLcA",
        "outputId": "809c8fb8-b5c1-4662-c7f9-31f417d2feaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2500, 256, 256, 1)\n",
            "(2500, 256, 256, 1)\n",
            "(2500, 256, 256, 1)\n",
            "(2500, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkJPrqTECLcA",
        "outputId": "d8d1e585-a08d-4ed6-eae1-215d423dc3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = sm.Unet(BACKBONE, input_shape= (256, 256, 1), encoder_weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q1X2D4oCLcC"
      },
      "outputs": [],
      "source": [
        "#opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# model.compile(\n",
        "#     'rmsprop',\n",
        "#     loss=sm.losses.bce_jaccard_loss,\n",
        "#     metrics=[sm.metrics.iou_score])\n",
        "\n",
        "# # fit model\n",
        "# # if you use data generator use model.fit_generator(...) instead of model.fit(...)\n",
        "# # more about `fit_generator` here: https://keras.io/models/sequential/#fit_generator\n",
        "# model.fit(\n",
        "#    x=X_train,\n",
        "#    y=y_train,\n",
        "#    batch_size=16,\n",
        "#    epochs=5,\n",
        "#    validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9jMBoJICLcD"
      },
      "outputs": [],
      "source": [
        "# model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdLc6nirCLcD"
      },
      "outputs": [],
      "source": [
        "# from keras.models import model_from_json\n",
        "# model_json = model.to_json()\n",
        "\n",
        "# with open(\"model.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "# model.save_weights(\"model.h5\")\n",
        "# print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx9eF-oiCLcE",
        "outputId": "935272a0-8f80-485b-d0c2-aa4b720a320f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2500, 256, 256, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_weights('./model.h5')\n",
        "print(X_test.shape)\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRjAVweuCLcE",
        "outputId": "276a5f84-b394-4e78-985b-9e904d7d9a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2500, 256, 256, 1)\n",
            "(2500, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "print(y_pred_train.shape)\n",
        "print(y_pred_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buxn88kACLcF"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_train)):\n",
        "#     print(i)\n",
        "#     print(i.shape)\n",
        "    if i == 250:\n",
        "        i = np.reshape(y_train[i], (256,256))\n",
        "        plt.imshow(i, cmap = plt.cm.gray)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIwOsI0NCLcG"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_pred_train)):\n",
        "#     print(i)\n",
        "#     print(i.shape)\n",
        "    if i == 250:\n",
        "        i = np.reshape(y_pred_train[i], (256,256))\n",
        "        plt.imshow(i, cmap = plt.cm.gray)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_BpS4M7CLcH"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_test)):\n",
        "#     print(i)\n",
        "#     print(i.shape)\n",
        "    if i == 150:\n",
        "        i = np.reshape(y_test[i], (256,256))\n",
        "        plt.imshow(i, cmap = plt.cm.gray)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-eJh9DPCLcH"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_pred_test)):\n",
        "#     print(i)\n",
        "#     print(i.shape)\n",
        "    if i == 150:\n",
        "        i = np.reshape(y_pred_test[i], (256,256))\n",
        "        plt.imshow(i, cmap = plt.cm.gray)\n",
        "        break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk6krJ7PCLcH"
      },
      "outputs": [],
      "source": [
        "for i in range(len(X_test)):\n",
        "#     print(i)\n",
        "#     print(i.shape)\n",
        "    if i == 150:\n",
        "        i = np.reshape(X_test[i], (256,256))\n",
        "        plt.imshow(i, cmap = plt.cm.gray)\n",
        "        break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuiYmd_7CLcI",
        "outputId": "500bbdd1-95ed-4605-99f5-1c704c0994f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "007\n",
            "validation\n",
            "155\n",
            "(240, 240, 155)\n"
          ]
        }
      ],
      "source": [
        "# lets build a segmentation \n",
        "val_path = \"/floyd/home/validation/BraTS20_Validation_007/BraTS20_Validation_007_flair.nii.gz\"\n",
        "dataIndex = val_path.split(\"/\")[4].split(\"_\")[2]\n",
        "dataType = val_path.split(\"/\")[3]\n",
        "print(dataIndex)\n",
        "print(dataType)\n",
        "\n",
        "image_test_seg = nibabel.load(val_path)\n",
        "print(image_test_seg.shape[2])\n",
        "print(image_test_seg.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBN0ElFdCLcI"
      },
      "outputs": [],
      "source": [
        "def get_coordinates(mat):\n",
        "\n",
        "    create_list = []\n",
        "    for idx, row in enumerate(mat):\n",
        "        val = next((i for i, x in enumerate(row) if x), 0.0) # x!= 0 for strict match\n",
        "        if val != 0:\n",
        "            #print(\"index: \", idx)\n",
        "            create_list.append(idx)\n",
        "\n",
        "    return create_list[0], create_list[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_tAgbvnCLcI",
        "outputId": "08cd0b94-aea8-47cc-b9ba-bc9183815cc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
            "\n",
            "* deprecated from version: 3.0\n",
            "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n"
          ]
        }
      ],
      "source": [
        "# typical output of the following function\n",
        "\"\"\"126: (88, 114, 149, 179, 780),\n",
        " 127: (90, 113, 151, 175, 552),\n",
        " 128: (92, 106, 155, 172, 238),\n",
        " 129: (95, 102, 160, 168, 56),\n",
        " 130: (0, 0, 0, 0, 0),\n",
        " 131: (0, 0, 0, 0, 0),\n",
        " 132: (0, 0, 0, 0, 0),\n",
        " 133: (0, 0, 0, 0, 0),\"\"\"\n",
        "def create_dic(val_path):\n",
        "    # path to the validation file\n",
        "    image_test_seg = nibabel.load(val_path)\n",
        "    \n",
        "    create_list = []\n",
        "    # area dictionary\n",
        "    area_dic = {}\n",
        "    # zero areas\n",
        "    area_zero = 0\n",
        "    \n",
        "    for k in range(0,image_test_seg.shape[2]):\n",
        "        image_seg = np.array(image_test_seg.get_data()[::2, ::2, k])\n",
        "        image_seg = cv2.resize(image_seg, (256, 256)) \n",
        "        #print(image_seg.shape)\n",
        "        #plt.imshow(image_seg, cmap = plt.cm.gray)\n",
        "        image_seg = np.expand_dims(image_seg, axis=-1)\n",
        "        #print(image_seg.shape)\n",
        "        image_seg = np.expand_dims(image_seg, axis=0)\n",
        "        #print(image_seg.shape)\n",
        "\n",
        "        seg_pred_ = model.predict(image_seg)\n",
        "        seg_pred_ = np.reshape(seg_pred_, (256,256))\n",
        "\n",
        "        #convet to int\n",
        "        seg_pred_ = np.int_(seg_pred_)\n",
        "        #print(seg_pred_)\n",
        "        seg_pred_zero = np.sum(seg_pred_)\n",
        "    \n",
        "        if seg_pred_zero <= 1:\n",
        "            area_dic.update({k:(0,0,0,0,area_zero)})\n",
        "        else:\n",
        "            row1, row2 = get_coordinates(seg_pred_)\n",
        "            col1, col2 = get_coordinates(seg_pred_.T)\n",
        "\n",
        "            area = abs(row1-row2)*abs(col1-col2)\n",
        "            #print(area)\n",
        "            area_dic.update({k:(row1, row2, col1, col2, area)})\n",
        "        \n",
        "    return area_dic\n",
        "area_dic = create_dic(val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42Z1gmmpCLcJ"
      },
      "outputs": [],
      "source": [
        "## zeros are eleminated\n",
        "\"\"\"126: (88, 114, 149, 179, 780),\n",
        " 127: (90, 113, 151, 175, 552),\n",
        " 128: (92, 106, 155, 172, 238),\n",
        " 129: (95, 102, 160, 168, 56)\"\"\"\n",
        "def clean_dic(area_dic):\n",
        "    area_dic_output = {k: v for k, v in area_dic.items() if all(i != 0 for i in v)}\n",
        "    return area_dic_output\n",
        "area_dic_output = clean_dic(area_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmBjyMo8CLcJ"
      },
      "outputs": [],
      "source": [
        "#typical output\n",
        "\"\"\"[55, 77, 78, 79, 80, 81, 95, 96, 97..... \"\"\"\n",
        "def get_key_area_dic_output(area_dic_output):\n",
        "    list_ = []\n",
        "    for key, values in area_dic_output.items():\n",
        "        list_.append(key)\n",
        "    return list_\n",
        "list_ = get_key_area_dic_output(area_dic_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQhIXorVCLcK"
      },
      "outputs": [],
      "source": [
        "## help us to get all the sub-dictionaries\n",
        "def sub_dict(list_):\n",
        "    list_ = sorted(set(list_))\n",
        "    # get gaps\n",
        "    g = [[s, e] for s, e in zip(list_, list_[1:]) if s+1 < e]\n",
        "    # get edges\n",
        "    ed = iter(list_[:1] + sum(g, []) + list_[-1:])\n",
        "    return list(zip(ed, ed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCalkHf7CLcK",
        "outputId": "626af441-ca26-4242-a93e-122fbb5fff58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(55, 55), (77, 81), (95, 129)]\n",
            "[(77, 81), (95, 129)]\n"
          ]
        }
      ],
      "source": [
        "get_all_dic = sub_dict(list_)\n",
        "print(get_all_dic)\n",
        "how_Many_dict = []\n",
        "for tup in get_all_dic:\n",
        "    # avoid dic with single non-zero lines from area_dic\n",
        "    if tup[0]!=tup[1]:\n",
        "        how_Many_dict.append(tup)\n",
        "print(how_Many_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSl_rRHlCLcK",
        "outputId": "cfd22880-7f9b-4866-9247-b11321a9d0fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(77, 81), (95, 129)]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "how_Many_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFn84GKbCLcL",
        "outputId": "6b51d497-6347-43cb-e754-f4e9bbdca0a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(77, 81), (95, 129)]\n"
          ]
        }
      ],
      "source": [
        "## dictionaries with the largest range\n",
        "# def dic_largest_range(how_Many_dict):\n",
        "#     # choose 1 or 2 to get the largest range (if more than 2 tumors)\n",
        "#     output = sorted(nlargest(2, how_Many_dict, key = lambda x: x[1] - x[0]))\n",
        "#     return output\n",
        "\n",
        "\n",
        "\n",
        "### We will change here to get only one BB!!\n",
        "\n",
        "def no_of_paires(how_Many_dict):\n",
        "    if len(how_Many_dict) == 1:\n",
        "        output = sorted(nlargest(1, how_Many_dict, key = lambda x: x[1] - x[0]))\n",
        "    elif len(how_Many_dict) == 2 :\n",
        "        output = sorted(nlargest(2, how_Many_dict, key = lambda x: x[1] - x[0]))\n",
        "    ## if we have more dic, pick the largest 2\n",
        "    elif len(how_Many_dict) > 2:\n",
        "        output = sorted(nlargest(2, how_Many_dict, key = lambda x: x[1] - x[0])) \n",
        "    return output\n",
        "\n",
        "output = no_of_paires(how_Many_dict)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD-MGHY-CLcL"
      },
      "outputs": [],
      "source": [
        "def midpoint(x1, y1, x2, y2):\n",
        "    return ((x1 + x2)/2, (y1 + y2)/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQekOd9vCLcL",
        "outputId": "f6dcd645-72b3-44be-a1ec-ae494b943f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79 112\n"
          ]
        }
      ],
      "source": [
        "# mid points\n",
        "cn_1 = int((how_Many_dict[0][0] + how_Many_dict[0][1])/2)\n",
        "cn_2 = int((how_Many_dict[1][0] + how_Many_dict[1][1])/2)\n",
        "print(cn_1, cn_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq9-oeMmCLcL"
      },
      "outputs": [],
      "source": [
        "def distinct_dic(output, area_dic_output):\n",
        "    \n",
        "    cube_dic1, cube_dic2, cube_dic3, cube_dic4, cube_dic5, cube_dic6 = {}, {}, {}, {}, {}, {}\n",
        "    cube_dic_ = {}\n",
        "\n",
        "    if len(output) == 2:\n",
        "        a = {}\n",
        "        b = {}\n",
        "        for key, value in area_dic_output.items():\n",
        "            if all(value):\n",
        "                if key in range(output[0][0], output[0][1]+1):\n",
        "                    a[key] = value\n",
        "                elif key in range(output[1][0], output[1][1]+1):\n",
        "                    b[key] = value\n",
        "        \n",
        "        k1, v1 = max(a.items(), key=lambda x:x[1][4])\n",
        "        len1 = abs(v1[0] - v1[1])\n",
        "        wid1 = abs(v1[2] - v1[3])\n",
        "        heigh1 = abs(output[0][0] - output[0][1])\n",
        "        #print(k1, v1)\n",
        "        k2, v2 = max(b.items(), key=lambda x:x[1][4])\n",
        "        \n",
        "        #print(k2, v2)\n",
        "        len2 = abs(v2[0] - v2[1])\n",
        "        wid2 = abs(v2[2] - v2[3])\n",
        "        heigh2 = abs(output[1][0] - output[1][1])\n",
        "        # mid points of the sequence (77, 78, 79, 80, 81) ----> 79 or any other \n",
        "        \n",
        "        # cn_1/cn_2 are the midpoints\n",
        "        cn_1 = int((output[0][0] + output[0][1])/2)\n",
        "        cn_2 = int((output[1][0] + output[1][1])/2)\n",
        "        \n",
        "        \n",
        "        # find the midpoint\n",
        "        c_a_x, c_a_y = midpoint(a.get(cn_1)[0], a.get(cn_1)[2], a.get(cn_1)[1],a.get(cn_1)[3])\n",
        "        c_b_x, c_b_y = midpoint(b.get(cn_2)[0],b.get(cn_2)[2],b.get(cn_2)[1],b.get(cn_2)[3])\n",
        "        \n",
        " ### create json file to feed it into the Unity!!       \n",
        "        foo = [\"x\",\"y\",\"z\"] \n",
        "        bar1 = [c_a_x, c_a_y, cn_1]\n",
        "        tar1 = [len1, wid1, heigh1]\n",
        "                        \n",
        "        bar2 = [c_b_x, c_b_y, cn_2]\n",
        "        tar2 = [len2, wid2, heigh2]\n",
        "        \n",
        "        for f, z in zip(foo, bar1):\n",
        "            cube_dic1.update({f:z})\n",
        "            \n",
        "        for f, z in zip(foo, tar1):\n",
        "            cube_dic2.update({f:z})\n",
        "        \n",
        "        for f, z in zip(foo, bar2):\n",
        "            cube_dic3.update({f:z})\n",
        "            \n",
        "        for f, z in zip(foo, tar2):\n",
        "            cube_dic4.update({f:z})\n",
        "           \n",
        "        tumorNum = len(output)\n",
        "        list_cube = [dataIndex, dataType, tumorNum, cube_dic1, cube_dic2, cube_dic3, cube_dic4]\n",
        "        \n",
        "        key = [\"dataIndex\", \"dataType\", \"tumorNum\",\"boxCenter\", \"boxDimension\", \"boxCenter2\", \"boxDimension2\"]\n",
        "        for w,c in zip(key, list_cube):\n",
        "            cube_dic_.update({w:c})\n",
        "        \n",
        "        #print(cube_dic5)\n",
        "        \n",
        "\n",
        "            \n",
        "#         cube_dic1.update({'boxCenter': (\"x\":c_a_x, \"y\":c_a_y, \"z\":cn_1)}) \n",
        "#         cube_dic2.update({'boxDimension': (\"x\":len1, \"y\":wid1, \"z\":heigh1)}) \n",
        "#         cube_dic3.update({'boxCenter2': (\"x\":c_b_x, \"y\":c_b_y, \"z\":cn_2)})  \n",
        "#         cube_dic4.update({'boxDimension2': (\"x\": len2, \"y\": wid2, \"z\": heigh2)})\n",
        "        \n",
        "#         cube_dic_.update({(cube_dic1, cube_dic2, cube_dic3, cube_dic4)})\n",
        "        # return x, y, z, length, width & height\n",
        "        #return (c_a_x, c_a_y, cn_1, len1, wid1, heigh1, c_b_x, c_b_y, cn_2, len2, wid2, heigh2)\n",
        "        \n",
        "        with open('result.json', 'w') as fp:\n",
        "            json.dump(cube_dic_, fp)\n",
        "        \n",
        "        return cube_dic_\n",
        "                \n",
        "    if len(output) == 1:\n",
        "        c = {}\n",
        "        for key, value in area_dic_output.items():\n",
        "            if all(value):\n",
        "                if key in range(output[0][0], output[0][1]+1):\n",
        "                    c[key] = value\n",
        "        k3, v3 = max(c.items(), key=lambda x:x[1][4])\n",
        "        len3 = abs(v3[0] - v3[1])\n",
        "        wid3 = abs(v3[2] - v3[3])\n",
        "        heigh3 = abs(output[0][0] - output[0][1])\n",
        "        \n",
        "        cn_3 = int((how_Many_dict[0][0] + how_Many_dict[0][1])/2)\n",
        "        c_c_x, c_c_y = midpoint(c.get(cn_3)[0], c.get(cn_3)[2], c.get(cn_3)[1],c.get(cn_3)[3])\n",
        "\n",
        " ### create json file to feed it into the Unity!!       \n",
        "        bar3 = [c_c_x, c_c_y, cn_3]\n",
        "        tar3 = [len3, wid3, heigh3]  \n",
        "        \n",
        "        for f, z in zip(foo, bar3):\n",
        "            cube_dic1.update({f:z})\n",
        "            \n",
        "        for f, z in zip(foo, tar3):\n",
        "            cube_dic2.update({f:z})\n",
        "        \n",
        "        tumorNum = len(output)\n",
        "        list_cube = [dataIndex, dataType, tumorNum, cube_dic1, cube_dic2]\n",
        "        \n",
        "        key = [\"dataIndex\", \"dataType\", \"tumorNum\",\"boxCenter\", \"boxDimension\"]\n",
        "        for w,c in zip(key, list_cube):\n",
        "            cube_dic_.update({w:c})        \n",
        "        \n",
        "        #return (c_c_x, c_c_y, cn_3, len3, wid3, heigh3)\n",
        "        with open('result.json', 'w') as fp:\n",
        "            json.dump(cube_dic_, fp)\n",
        "        \n",
        "        return cube_dic_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NevIpTcVCLcM",
        "outputId": "c527680b-741b-4471-a059-d21013359f8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dataIndex': '007',\n",
              " 'dataType': 'validation',\n",
              " 'tumorNum': 2,\n",
              " 'boxCenter': {'x': 73.0, 'y': 155.0, 'z': 79},\n",
              " 'boxDimension': {'x': 10, 'y': 10, 'z': 4},\n",
              " 'boxCenter2': {'x': 70.5, 'y': 148.5, 'z': 112},\n",
              " 'boxDimension2': {'x': 29, 'y': 53, 'z': 34}}"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "distinct_dic(output, area_dic_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVzmgll2CLcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7u7WIxrCLcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYpJHg9YCLcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nF5KeygCLcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SP92Km_CLcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYMLwFxNCLcO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp3CQkB6CLcO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djkLBTSECLcO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}